---
output: 
  html_document: 
    theme: journal
    toc: yes
---
```{r Libraries}
library(readr)
library(ggplot2)
library(ggthemes)
source('function_library.R')
source('kalman_2.R')
```

# Apply models to simulated data 

Simulate an autoregressive function + noise n times. Then run kalman filtering algorithm on it.
Results2 runs kalman filtering on a single simulation but varies the noise parameter W. 
$$ Y = 0.7Y_{t-1} + 0.3Y_{t-2} + \eta $$
```{r Simulation Data, message=FALSE, warning=FALSE}
#Guess parameters
ORDER <- 2
W = diag(ORDER) * 1e-7
# Initial state estimate
theta = rep(1/ORDER, ORDER)
P = diag(ORDER) * 100

# Run kalman filtering many times and cbind
ntimes <- 10
ntimes2 <- 5
results <- data.frame()
RMSE <- matrix(NA, nrow = ntimes, ncol = ntimes2 * 2)
for (i in 1:ntimes) {
    # Generate test data
    targets <- simulation(300, c(.3,.7), 1, .1)
    for (j in 1:ntimes2) {
        # Set tuning parameters
        V <- autoregression(targets, ORDER)$variance
        W = diag(ORDER) * 1 * 10 ^ ((j - 1 - round((ntimes2-1)/2)) * 2)
        
        kal <- kalman(targets, ORDER, V, W, theta, P)
        kal$simulation.id <- i
        kal$W <- W[1,1]
        kal$index <- 1:nrow(kal)
        results <- rbind(results, kal)
        
        #RMSE
        RMSE[i,j] <- sqrt(sum(autoregression(targets, ORDER)$residuals^2) / length(targets))
        RMSE[i,j + ntimes2] <- sqrt(sum( (kal$targets - kal$Estimate)^2, na.rm = T ) / length(targets))
    }
}
results <- na.omit(results)
results$simulation.id <- as.factor(results$simulation.id)
results$W <- as.factor(results$W)
```

## Make plots of simulation data 

```{r Weight approximation plot, message=FALSE, warning=FALSE}
colors <- RColorBrewer::brewer.pal(4, name = "Set1")

dat1 <- results[results$W == 0.0001,]
# Plot kalman filter approximation of weights
gg <- ggplot(data = dat1, aes(x = index, group = simulation.id)) +
    geom_line(aes(y = Theta1, size = simulation.id, alpha = simulation.id), col = colors[1]) +
    geom_line(aes(y = Theta2, size = simulation.id, alpha = simulation.id), col = colors[2]) +
    theme_few() +
    geom_hline(yintercept = .3, col = colors[2], linetype = 2, size = 1.5) +
    geom_hline(yintercept = .7, col = colors[1], linetype = 2, size = 1.5) +
    scale_y_continuous(breaks = seq(.1, 1, .1), limits = c(0,1)) +
    scale_size_manual(values = c(1.5, rep(1, ntimes - 1))) +
    scale_alpha_manual(values = c(1, rep(.2, ntimes - 1))) +
    ylab("Theta") +
    xlab("Day")

png("Test_Weight_Estimation.png")
gg
invisible(dev.off())
gg
```

```{r Kalman smoothing plot}
# Plot kalman filter smoothing of data
dat2 <- results[(results$index %in% 1:50) & results$simulation.id == "1" & results$W == 0.0001,]
gg2 <- ggplot(data = dat2, aes(x = index)) + 
    geom_line(aes(y = Estimate)) + 
    geom_point(aes(y = targets), col = "blue") + 
    ylab("y") +
    xlab("Day") +
    theme_few()

png("Test_Estimation.png")
gg2
invisible(dev.off())
gg2
```

### W variaiton plot
The shape of W also has an impact (but use diagonal matrix here)
Lower orders of magnitude are "smoother", on a static function like this one more smoothing improves filtering but takes longer to "lock on" to the function. For the real data where the underlying function is not static this can lead to "chasing the dragon" where the underlying function changes faster than it is locked onto. 
```{r W variation plot}
# variation in W order of magnitude
dat3 <- results[(results$index %in% 1:30) & results$simulation.id == "1",]
gg3 <- ggplot(data = dat3, aes(x = index, colour = W)) + 
    geom_point(aes(y = targets), col = "blue") + 
    geom_line(aes(y = Estimate)) +
    ylab("y") +
    xlab("Day") +
    theme_few() +
    scale_color_brewer(palette = "Set1")

png("W_variation.png")
gg3
invisible(dev.off())
gg3
```

# REAL DATA

```{r Read data} 
dat_raw <- read_csv("S&P500.csv")
targets <- (dat_raw$Close)
```

```{r Run kalman filtering / AR}
results_real <- data.frame()
ntimes2 = 20
w.vals <- (1 * 10) ^ (ceiling(-ntimes2/2):floor(ntimes2/2))
RMSE_real <- matrix(NA, nrow = ntimes2, ncol = length(w.vals) + 1)
for (i in 1:ntimes2) {
    # Guess parameters
    ORDER <- i
    AR_model <- autoregression(targets, ORDER)
    V <- AR_model$variance
    
    for (j in 1:length(w.vals)) {
        W = diag(ORDER) * w.vals[j]
        
        # Initial state estimate
        theta = rep(1/ORDER, ORDER)
        P = diag(ORDER) * 1e-3
        
        # Kalman
        kal <- kalman(targets, ORDER, V, W, theta, P)
        kal$index <- 1:nrow(kal)
        kal$order <- ORDER
        kal$W <- W[1,1]
        kal <- kal[,-c(grep("^P[0-9]*$", names(kal)))]
        kal <- kal[,-c(grep("^Theta[0-9]*$", names(kal)))]

        
        results_real <- rbind(results_real, kal)
        
        RMSE_real[i,j] <-  sqrt( sum((kal$targets - kal$Estimate)^2, na.rm = T) / length(targets) )
    }
    RMSE_real[i, length(w.vals) + 1] <- sqrt(sum(AR_model$residuals^2) / length(targets))
    
}
colnames(RMSE_real) <- c(paste0("w", w.vals), "AR")
rownames(RMSE_real) <- c(paste0("Order", 1:ntimes2))
```


## Plots


```{r Residual distribution AR vs kal}
# Plot residual distribution
gg5 <- qplot(kal$targets - kal$Estimate, alpha = .5) + 
    geom_histogram(aes(x = AR_model$residuals), fill = "blue", alpha = .5) +
    theme_few() +
    xlab("Residuals")

png("Residual_distribution.png")
gg5
invisible(dev.off())
gg5

print( paste("AR RMSE: ", sqrt( sum(AR_model$residuals^2)) / length(AR_model$residuals)))
print( paste("Kalman RMSE: ", sqrt( sum((kal$targets - kal$Estimate)^2, na.rm = T)) / length(kal$targets) ))
```

### SUMMARY OF ERRORS FOR AUTOREGRESSION AND KALMAN ON TEST / REAL DATA #################
#########################################################################################

### Plot rmse vs. order for a variety of orders, 2 plots one for real one for test. 
The order of the data is 2, when the order is lower than this the RMSE is significantly higher than other points. 
However after this point RMSE isn't significantly impacted by order because the model is able to capture the order. 
RMSE decreases generally for the more smoothing present. This is because it is static data. Smoothing can be created by lower W or higher order terms. 
```{r, eval=F}
### TO RUN THIS SNIPET FIRST GENERATE RMSE1:5 BY CHANGING ORDER FROM 1:5 AND RUNNING THE SIMULATION CODE

RMSEs <- list(Order1 = RMSE1[,5:10], Order2 = RMSE2[,5:10], Order3 = RMSE3[,5:10], Order4 = RMSE4[,5:10], Order5 = RMSE5[,5:10])
RMSEm <- sapply(RMSEs, function(x) apply(x, 2, mean))
RMSEsd <- sapply(RMSEs, function(x) apply(x, 2, sd))
#RMSEse <- RMSEsd / sqrt(10)

t1 <- tidyr::gather(as.data.frame(RMSEm), order, RMSE)
t2 <- tidyr::gather(as.data.frame(RMSEsd), order, stand_dev)
RMSE_df <- cbind(t1, t2)
RMSE_df$model <- c("AR", rep("Kalman", 5))
RMSE_df$W <- rep(c("NA", "1e-4", "1e-2", "1", "1e2", "1e4"),5)
RMSE_df$model <- as.factor(RMSE_df$model)
RMSE_df$W <- as.factor(RMSE_df$W)
RMSE_df <- RMSE_df[,-3]

gg6 <- ggplot(RMSE_df, aes(x = order, color = W)) +
    geom_errorbar(aes(ymin = RMSE - 2 * stand_dev / sqrt(nrow(RMSE_df)), 
                      ymax = RMSE + 2 * stand_dev / sqrt(nrow(RMSE_df))), width = .2) +
    geom_point(aes(y = RMSE)) +
    theme_few()

png("RMSE_order.png")
gg6
invisible(dev.off())
gg6
```

# We see a very similar pattern for the real data.
* All values above 0.01 converge to the same place (assume this means able to respond fully to the change from observation)
* Values below 10 ^ - 6 converge to the same place
* Never reachest the level of AR
* Order 1 with low W value has best performance (but gets close)
* High W is better than low W for orders >= 3 (basically high order and low W are too different ways to achieve smoothing)
It appears that a best fit for the data is high values of W and an order of 

```{r Real data, message=FALSE, warning=FALSE}
# Repeat the above plot for real data
RMSE_real_df <- tidyr::gather(as.data.frame(t(RMSE_real)), order, RMSE)
RMSE_real_df$W <- rep( c(w.vals, "AR"), nrow(RMSE_real))
RMSE_real_df$order <- as.numeric(gsub("Order", "", RMSE_real_df$order))
RMSE_real_df <- RMSE_real_df[RMSE_real_df$order %in% 1:10, ]

gg7 <- ggplot(RMSE_real_df, aes(x = jitter(order), color = W)) +
    geom_point(aes(y = RMSE)) + 
    theme_few() +
    scale_x_continuous(breaks = 1:10)

png("RMSE_order_real.png")
gg7
invisible(dev.off())
gg7
```

```{r Different models, message=FALSE, warning=FALSE}
results_real$order <- as.factor(results_real$order)
gg8 <- ggplot(data = results_real[results_real$W == 1e-7 & results_real$order == 1,], aes(x = index)) + 
    geom_point(aes(y = targets)) +
    geom_line(aes(y = Estimate), col = colors[1], size = 1.5, alpha = .5) +
    geom_line(aes(y = results_real[results_real$W == 1e5 & results_real$order == 5, "Estimate"]),
              col = colors[2], size = 1.5, alpha = .5) +
    geom_line(aes(y = c(rep(NA, 20), AR_model$predictions)), col = colors[3], size = 1.5, alpha = .5) +
    scale_x_continuous(limits = c(50,95)) +
    scale_y_continuous(limits = c( 700, 1500)) +
    labs(x = "Month", y = "Index Value") +
    theme_few()
# Red = order 1, w 1e-7, Blue order = 5, w 1e5, green = AR order = 3
png("DifferentModels.png")
gg8
invisible(dev.off())
gg8
```

```{r Lasso Dataframe}
# Produce a dataframe of residuals for each model
Residuals <- data.frame(targets = targets)
Residuals$Kalman1 <- targets - kalman(targets, 1, V, diag(1) * 1e-7, rep(1/2, 1), P = diag(1) * 0.1)$Estimate
Residuals$Kalman5 <- targets - kalman(targets, 5, V, diag(5) * 1e5, rep(1/2, 5), P = diag(5) * 0.1)$Estimate
Residuals$Kalman10 <- targets - kalman(targets, 10, V, diag(10) * 1e5, rep(1/2, 10), P = diag(10) * 0.1)$Estimate
Residuals$AR3 <- c(rep(NA, 3), autoregression(targets, 3)$residuals)
Residuals$ARIMA011 <- arima(targets, c(0L, 1L, 1L))$residuals
Residuals$ARIMA022 <- arima(targets, c(0L, 2L, 2L))$residuals

saveRDS(Residuals, "Residuals.rds")
```